{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except ImportError:\n",
    "    xgb = None\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data():\n",
    "    \"\"\"\n",
    "    Task 1: Data Loading and Cleaning\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TASK 1: DATA LOADING AND CLEANING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df = pd.read_excel(\"test.xlsx\")\n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "    \n",
    "    df = df.dropna(subset=['body', 'date', 'from'])\n",
    "    df = df[df['body'].str.strip() != '']\n",
    "    print(f\"After removing missing data: {df.shape}\")\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df = df.dropna(subset=['date'])  # type: ignore\n",
    "    print(f\"After date conversion: {df.shape}\")\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['from', 'date', 'body'])\n",
    "    print(f\"After removing duplicates: {df.shape}\")\n",
    "    \n",
    "    def clean_message(text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[\\r\\n\\t]+', ' ', text)\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    df['cleaned_message'] = df['body'].apply(clean_message)\n",
    "    df['word_count'] = df['cleaned_message'].str.split().str.len()\n",
    "    df['char_count'] = df['cleaned_message'].str.len()\n",
    "    \n",
    "    print(\"Data cleaning completed!\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_sentiment_labeling(df):\n",
    "    \"\"\"\n",
    "    Task 1: Sentiment Labeling\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TASK 1: SENTIMENT LABELING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def get_sentiment(text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"Neutral\"\n",
    "        \n",
    "        score = analyzer.polarity_scores(text)\n",
    "        compound = score['compound']\n",
    "        \n",
    "        if compound >= 0.05:\n",
    "            return 'Positive'\n",
    "        elif compound <= -0.05:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "    \n",
    "    \n",
    "    df['sentiment_label'] = df['cleaned_message'].apply(get_sentiment)\n",
    "    \n",
    "    \n",
    "    sentiment_score_map = {\n",
    "        'Positive': 1,\n",
    "        'Neutral': 0,\n",
    "        'Negative': -1\n",
    "    }\n",
    "    df['sentiment_score'] = df['sentiment_label'].map(sentiment_score_map)\n",
    "    \n",
    "    print(\"Sentiment labeling completed!\")\n",
    "    print(f\"Sentiment distribution:\\n{df['sentiment_label'].value_counts()}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploratory_data_analysis(df):\n",
    "    \"\"\"\n",
    "    Task 2: Exploratory Data Analysis (EDA)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TASK 2: EXPLORATORY DATA ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    \n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.rcParams['figure.figsize'] = (12, 6)\n",
    "    \n",
    "    print(\"Dataset Information:\")\n",
    "    print(df.info())\n",
    "    print(f\"\\nMissing Values:\\n{df.isnull().sum()}\")\n",
    "    \n",
    "    sentiment_counts = df['sentiment_label'].value_counts()\n",
    "    print(f\"\\nSentiment Distribution:\\n{sentiment_counts}\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    sns.countplot(data=df, x='sentiment_label', ax=axes[0,0], palette='Set2')\n",
    "    axes[0,0].set_title(\"Sentiment Label Distribution\")\n",
    "    axes[0,0].set_xlabel(\"Sentiment\")\n",
    "    axes[0,0].set_ylabel(\"Number of Messages\")\n",
    "    \n",
    "    sentiment_counts.plot.pie(autopct='%1.1f%%', startangle=90, \n",
    "                             colors=sns.color_palette(\"Set2\"), ax=axes[0,1])\n",
    "    axes[0,1].set_title(\"Sentiment Distribution\")\n",
    "    axes[0,1].set_ylabel(\"\")\n",
    "    \n",
    "    df['month'] = df['date'].dt.to_period('M')\n",
    "    monthly_trend = df.groupby(['month', 'sentiment_label']).size().unstack(fill_value=0)\n",
    "    monthly_trend.plot(marker='o', ax=axes[1,0])\n",
    "    axes[1,0].set_title(\"Monthly Sentiment Trend\")\n",
    "    axes[1,0].set_xlabel(\"Month\")\n",
    "    axes[1,0].set_ylabel(\"Message Count\")\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    axes[1,0].grid(True)\n",
    "    axes[1,0].legend(title=\"Sentiment\")\n",
    "    \n",
    "    sns.histplot(df['word_count'], bins=30, kde=True, color='purple', ax=axes[1,1])\n",
    "    axes[1,1].set_title(\"Distribution of Word Counts in Messages\")\n",
    "    axes[1,1].set_xlabel(\"Word Count\")\n",
    "    axes[1,1].set_ylabel(\"Frequency\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualization/eda_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    top_senders = df['from'].value_counts().head(10)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(y=top_senders.index, x=top_senders.values, palette=\"coolwarm\")\n",
    "    plt.title(\"Top 10 Employees by Message Count\")\n",
    "    plt.xlabel(\"Messages Sent\")\n",
    "    plt.ylabel(\"Employee\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualization/top_employees.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"EDA completed and visualizations saved!\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_employee_scores(df):\n",
    "    \"\"\"\n",
    "    Task 3: Employee Score Calculation\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TASK 3: EMPLOYEE SCORE CALCULATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    monthly_scores = df.groupby(['from', df['date'].dt.to_period('M')])['sentiment_score'].sum().reset_index()\n",
    "    monthly_scores.columns = ['employee', 'month', 'monthly_sentiment_score']\n",
    "    monthly_scores = monthly_scores.sort_values(['employee', 'month'])\n",
    "    \n",
    "    print(f\"Monthly scores calculated for {len(monthly_scores)} employee-month combinations\")\n",
    "    print(f\"Score range: {monthly_scores['monthly_sentiment_score'].min()} to {monthly_scores['monthly_sentiment_score'].max()}\")\n",
    "    \n",
    "    monthly_scores.to_excel(\"D:\\\\employee-sentiment-analysis\\\\monthly_employee_sentiment_scores.xlsx\", index=False)\n",
    "    print(\"Monthly scores saved to 'monthly_employee_sentiment_scores.xlsx'\")\n",
    "    \n",
    "    return monthly_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def employee_ranking(monthly_scores):\n",
    "    \"\"\"\n",
    "    Task 4: Employee Ranking\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TASK 4: EMPLOYEE RANKING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    top_positive = (\n",
    "        monthly_scores\n",
    "        .sort_values(['month', 'monthly_sentiment_score', 'employee'], ascending=[True, False, True])\n",
    "        .groupby('month')\n",
    "        .head(3)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    top_positive['rank_type'] = 'Top Positive'\n",
    "    \n",
    "    top_negative = (\n",
    "        monthly_scores\n",
    "        .sort_values(['month', 'monthly_sentiment_score', 'employee'], ascending=[True, True, True])\n",
    "        .groupby('month')\n",
    "        .head(3)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    top_negative['rank_type'] = 'Top Negative'\n",
    "    \n",
    "    \n",
    "    employee_ranking = pd.concat([top_positive, top_negative], ignore_index=True)\n",
    "    employee_ranking = employee_ranking.sort_values(['month', 'rank_type', 'monthly_sentiment_score'], \n",
    "                                                   ascending=[True, True, False])\n",
    "    \n",
    "    print(\"Top 3 Positive and Negative Employees per Month:\")\n",
    "    print(employee_ranking.head(20))\n",
    "    \n",
    "    \n",
    "    employee_ranking.to_excel(\"D:\\\\employee-sentiment-analysis\\\\employee_monthly_ranking.xlsx\", index=False)\n",
    "    print(\"Employee rankings saved to 'employee_monthly_ranking.xlsx'\")\n",
    "    \n",
    "    return employee_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flight_risk_identification(df):\n",
    "    \"\"\"\n",
    "    Task 5: Flight Risk Identification\n",
    "    CORRECTED: According to project requirements - 4+ negative messages in a given month\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TASK 5: FLIGHT RISK IDENTIFICATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    neg_df = df[df['sentiment_label'] == 'Negative'].copy()\n",
    "    neg_df['date'] = pd.to_datetime(neg_df['date'], errors='coerce')\n",
    "    \n",
    "    monthly_negative_counts = neg_df.groupby(['from', neg_df['date'].dt.to_period('M')]).size().reset_index(name='negative_count')\n",
    "    \n",
    "    flight_risk_employees = monthly_negative_counts[monthly_negative_counts['negative_count'] >= 4]['from'].unique()\n",
    "    flight_risks = set(flight_risk_employees)\n",
    "    \n",
    "    print(f\"Found {len(flight_risks)} employees flagged as flight risks\")\n",
    "    print(\"Flight Risk Employees:\")\n",
    "    for emp in flight_risks:\n",
    "        print(f\"- {emp}\")\n",
    "    \n",
    "    \n",
    "    flight_risk_df = pd.DataFrame({'employee': list(flight_risks)})\n",
    "    flight_risk_df.to_excel(\"D:\\\\employee-sentiment-analysis\\\\flight_risk_employees.xlsx\", index=False)\n",
    "    print(\"Flight risk list saved to 'flight_risk_employees.xlsx'\")\n",
    "    \n",
    "    return flight_risk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictive_modeling(df):\n",
    "    \"\"\"\n",
    "    Task 6: Enhanced Predictive Modeling with Advanced Features and Models\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TASK 6: ENHANCED PREDICTIVE MODELING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    \n",
    "    print(\"Creating enhanced features...\")\n",
    "    \n",
    "    \n",
    "    monthly_features = df.groupby(['from', df['date'].dt.to_period('M')]).agg({\n",
    "        'cleaned_message': 'count',\n",
    "        'word_count': ['sum', 'mean', 'std'],\n",
    "        'char_count': ['sum', 'mean'],\n",
    "        'sentiment_score': ['sum', 'mean', 'std'],\n",
    "        'sentiment_label': lambda x: (x == 'Positive').sum(),\n",
    "        'body': lambda x: (x == 'Negative').sum()\n",
    "    }).reset_index()\n",
    "    \n",
    "   \n",
    "    monthly_features.columns = ['employee', 'month', 'message_count', 'total_word_count', \n",
    "                               'avg_word_count', 'word_count_std', 'total_char_count', \n",
    "                               'avg_char_count', 'sentiment_score_sum', 'sentiment_score_mean', \n",
    "                               'sentiment_score_std', 'positive_count', 'negative_count']\n",
    "    \n",
    "   \n",
    "    monthly_features['sentiment_ratio'] = monthly_features['sentiment_score_sum'] / monthly_features['message_count']\n",
    "    monthly_features['positive_ratio'] = monthly_features['positive_count'] / monthly_features['message_count']\n",
    "    monthly_features['negative_ratio'] = monthly_features['negative_count'] / monthly_features['message_count']\n",
    "    monthly_features['word_per_message'] = monthly_features['total_word_count'] / monthly_features['message_count']\n",
    "    monthly_features['char_per_word'] = monthly_features['total_char_count'] / monthly_features['total_word_count']\n",
    "    \n",
    "  \n",
    "    monthly_features['month_num'] = monthly_features['month'].astype(str).str[:4].astype(int) * 12 + \\\n",
    "                                   monthly_features['month'].astype(str).str[5:7].astype(int)\n",
    "    \n",
    "   \n",
    "    employee_stats = df.groupby('from').agg({\n",
    "        'sentiment_score': ['mean', 'std'],\n",
    "        'word_count': ['mean', 'std'],\n",
    "        'cleaned_message': 'count'\n",
    "    }).reset_index()\n",
    "    employee_stats.columns = ['employee', 'emp_sentiment_mean', 'emp_sentiment_std', \n",
    "                             'emp_word_mean', 'emp_word_std', 'emp_total_messages']\n",
    "    \n",
    "    monthly_features = monthly_features.merge(employee_stats, on='employee', how='left')\n",
    "    \n",
    "    monthly_features['sentiment_volatility'] = monthly_features['sentiment_score_std'] / (monthly_features['emp_sentiment_std'] + 1e-6)\n",
    "    monthly_features['word_volatility'] = monthly_features['word_count_std'] / (monthly_features['emp_word_std'] + 1e-6)\n",
    "    monthly_features['message_intensity'] = monthly_features['message_count'] / monthly_features['emp_total_messages']\n",
    "    \n",
    "    monthly_features.dropna(inplace=True)\n",
    "    \n",
    "    print(f\"Enhanced feature dataset shape: {monthly_features.shape}\")\n",
    "    \n",
    "    feature_columns = ['message_count', 'total_word_count', 'avg_word_count', 'word_count_std',\n",
    "                      'total_char_count', 'avg_char_count', 'sentiment_score_sum', 'sentiment_score_mean', \n",
    "                      'sentiment_score_std', 'positive_count', 'negative_count', 'sentiment_ratio',\n",
    "                      'positive_ratio', 'negative_ratio', 'word_per_message', 'char_per_word',\n",
    "                      'month_num', 'emp_sentiment_mean', 'emp_sentiment_std', 'emp_word_mean', \n",
    "                      'emp_word_std', 'emp_total_messages', 'sentiment_volatility', 'word_volatility', \n",
    "                      'message_intensity']\n",
    "    \n",
    "    X = monthly_features[feature_columns]\n",
    "    y = monthly_features['sentiment_score_sum']  # Target: monthly sentiment score\n",
    "    \n",
    "    print(f\"Number of features: {len(feature_columns)}\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    \n",
    "    selector = SelectKBest(score_func=f_regression, k=15)\n",
    "    X_selected = selector.fit_transform(X_scaled, y)\n",
    "    selected_features = X_scaled.columns[selector.get_support()].tolist()\n",
    "    X_selected = pd.DataFrame(X_selected, columns=selected_features)  # type: ignore\n",
    "    \n",
    "    print(f\"Selected top {len(selected_features)} features: {selected_features}\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(alpha=1.0),\n",
    "        'Lasso Regression': Lasso(alpha=0.1),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        'SVR': SVR(kernel='rbf', C=1.0, gamma='scale')\n",
    "    }\n",
    "    \n",
    "   \n",
    "    if xgb is not None:\n",
    "        models['XGBoost'] = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "    \n",
    "    results = {}\n",
    "    best_model = None\n",
    "    best_r2 = -float('inf')\n",
    "    best_name = \"\"\n",
    "    \n",
    "    print(\"\\nTraining and evaluating models...\")\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            \n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            \n",
    "            results[name] = {'r2': r2, 'rmse': rmse, 'model': model}\n",
    "            \n",
    "            print(f\"{name}: RÂ² = {r2:.3f}, RMSE = {rmse:.3f}\")\n",
    "            \n",
    "            if r2 > best_r2:\n",
    "                best_r2 = r2\n",
    "                best_model = model\n",
    "                best_name = name\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nPerforming hyperparameter tuning for {best_name}...\")\n",
    "    \n",
    "    if best_name == 'Random Forest':\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    elif best_name == 'XGBoost':\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 6, 9],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    elif best_name == 'Gradient Boosting':\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 6, 9],\n",
    "            'learning_rate': [0.01, 0.1, 0.2]\n",
    "        }\n",
    "    else:\n",
    "        param_grid = {}\n",
    "    \n",
    "    if param_grid:\n",
    "        grid_search = GridSearchCV(best_model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_r2 = grid_search.best_score_\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    if best_model is not None:\n",
    "        y_pred_final = best_model.predict(X_test)\n",
    "        final_r2 = r2_score(y_test, y_pred_final)\n",
    "        final_rmse = np.sqrt(mean_squared_error(y_test, y_pred_final))\n",
    "    else:\n",
    "        fallback_model = LinearRegression()\n",
    "        fallback_model.fit(X_train, y_train)\n",
    "        y_pred_final = fallback_model.predict(X_test)\n",
    "        final_r2 = r2_score(y_test, y_pred_final)\n",
    "        final_rmse = np.sqrt(mean_squared_error(y_test, y_pred_final))\n",
    "        best_model = fallback_model\n",
    "        best_name = \"Linear Regression (Fallback)\"\n",
    "    \n",
    "    print(f\"\\nFinal Model Performance ({best_name}):\")\n",
    "    print(f\"RÂ² Score: {final_r2:.3f}\")\n",
    "    print(f\"RMSE: {final_rmse:.3f}\")\n",
    "    \n",
    "    if best_model is not None and hasattr(best_model, 'feature_importances_') and not isinstance(best_model, LinearRegression):\n",
    "        try:\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': selected_features,\n",
    "                'Importance': best_model.feature_importances_\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\nTop 10 Feature Importances:\")\n",
    "            print(importance_df.head(10))\n",
    "            \n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            importance_df.head(10).plot(x='Feature', y='Importance', kind='barh')\n",
    "            plt.title(f'Top 10 Feature Importances - {best_name}')\n",
    "            plt.xlabel('Importance')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('visualization/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Could not plot feature importance: {e}\")\n",
    "    else:\n",
    "        print(\"\\nFeature importance not available for this model type\")\n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    " \n",
    "    model_names = list(results.keys())\n",
    "    r2_scores = [results[name]['r2'] for name in model_names]\n",
    "    \n",
    "    axes[0,0].bar(model_names, r2_scores, color='skyblue')\n",
    "    axes[0,0].set_title('Model RÂ² Score Comparison')\n",
    "    axes[0,0].set_ylabel('RÂ² Score')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    " \n",
    "    axes[0,1].scatter(y_test, y_pred_final, alpha=0.6, color='green')\n",
    "    y_test_min, y_test_max = float(y_test.min()), float(y_test.max())  # type: ignore\n",
    "    axes[0,1].plot([y_test_min, y_test_max], [y_test_min, y_test_max], 'r--', lw=2)\n",
    "    axes[0,1].set_xlabel('Actual Sentiment Score')\n",
    "    axes[0,1].set_ylabel('Predicted Sentiment Score')\n",
    "    axes[0,1].set_title(f'Best Model Predictions ({best_name})\\nRÂ² = {final_r2:.3f}')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    \n",
    "    residuals = y_test - y_pred_final\n",
    "    axes[1,0].scatter(y_pred_final, residuals, alpha=0.6, color='orange')\n",
    "    axes[1,0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[1,0].set_xlabel('Predicted Values')\n",
    "    axes[1,0].set_ylabel('Residuals')\n",
    "    axes[1,0].set_title('Residuals Plot')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    \n",
    "    axes[1,1].hist(residuals, bins=20, alpha=0.7, color='purple', edgecolor='black')\n",
    "    axes[1,1].set_xlabel('Residuals')\n",
    "    axes[1,1].set_ylabel('Frequency')\n",
    "    axes[1,1].set_title('Residuals Distribution')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualization/enhanced_model_performance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    if best_model is not None:\n",
    "        cv_scores = cross_val_score(best_model, X_selected, y, cv=5, scoring='r2')\n",
    "        print(f\"\\nCross-validation RÂ² scores: {cv_scores}\")\n",
    "        print(f\"Mean CV RÂ²: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "    else:\n",
    "        print(\"\\nCross-validation skipped due to model training issues\")\n",
    "    \n",
    "    return best_model, final_r2, final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_report(monthly_scores, flight_risk_df, model_r2):\n",
    "    \"\"\"\n",
    "    Create summary report for README\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CREATING SUMMARY REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    \n",
    "    overall_scores = monthly_scores.groupby('employee')['monthly_sentiment_score'].mean().sort_values(ascending=False)\n",
    "    \n",
    "    top_3_positive = overall_scores.head(3)\n",
    "    top_3_negative = overall_scores.tail(3)\n",
    "    \n",
    "    print(\"Overall Top 3 Positive Employees:\")\n",
    "    for i, (emp, score) in enumerate(top_3_positive.items(), 1):\n",
    "        print(f\"{i}. {emp} (Score: {score:.1f})\")\n",
    "    \n",
    "    print(\"\\nOverall Top 3 Negative Employees:\")\n",
    "    for i, (emp, score) in enumerate(top_3_negative.items(), 1):\n",
    "        print(f\"{i}. {emp} (Score: {score:.1f})\")\n",
    "    \n",
    "    print(f\"\\nFlight Risk Employees ({len(flight_risk_df)} total):\")\n",
    "    for emp in flight_risk_df['employee']:\n",
    "        print(f\"- {emp}\")\n",
    "    \n",
    "    print(f\"\\nModel Performance: RÂ² = {model_r2:.3f}\")\n",
    "    \n",
    "    readme_content = f\"\"\"# Employee Sentiment & Flight Risk Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This project analyzes employee communication data to identify sentiment trends and potential flight risks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Top 3 Positive Employees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. {top_3_positive.index[0]} (Score: {top_3_positive.iloc[0]:.1f})\n",
    "2. {top_3_positive.index[1]} (Score: {top_3_positive.iloc[1]:.1f})\n",
    "3. {top_3_positive.index[2]} (Score: {top_3_positive.iloc[2]:.1f})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Top 3 Negative Employees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. {top_3_negative.index[0]} (Score: {top_3_negative.iloc[0]:.1f})\n",
    "2. {top_3_negative.index[1]} (Score: {top_3_negative.iloc[1]:.1f})\n",
    "3. {top_3_negative.index[2]} (Score: {top_3_negative.iloc[2]:.1f})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Employees Flagged as Flight Risks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "    <br>\n",
    "    for emp in flight_risk_df['employee']:<br>\n",
    "        readme_content += f\"- {emp}\\n\"<br>\n",
    "    <br>\n",
    "  readme_content += f\n",
    "<br>\n",
    "#  Key Insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Model Performance: RÂ² = {model_r2:.3f}\n",
    "- {len(flight_risk_df)} employees identified as flight risks\n",
    "- Sentiment analysis completed using VADER\n",
    "- Monthly scoring system implemented\n",
    "- Predictive modeling with linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Files Generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- `monthly_employee_sentiment_scores.xlsx`: Monthly sentiment scores\n",
    "- `employee_monthly_ranking.xlsx`: Monthly employee rankings\n",
    "- `flight_risk_employees.xlsx`: Flight risk employee list\n",
    "- `visualization/`: Charts and graphs\n",
    "\"\"\"\n",
    "    \n",
    "    with open('README.md', 'w', encoding='utf-8') as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    print(\"README.md updated with summary!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    print(\"EMPLOYEE SENTIMENT ANALYSIS PROJECT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    \n",
    "    import os\n",
    "    os.makedirs('visualization', exist_ok=True)\n",
    "    \n",
    "    df = load_and_clean_data()\n",
    "    df = perform_sentiment_labeling(df)\n",
    "    df = exploratory_data_analysis(df)\n",
    "    monthly_scores = calculate_employee_scores(df)\n",
    "    employee_ranking(monthly_scores)\n",
    "    flight_risk_df = flight_risk_identification(df)\n",
    "    model, r2, rmse = predictive_modeling(df)\n",
    "    create_summary_report(monthly_scores, flight_risk_df, r2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"All tasks completed according to project requirements:\")\n",
    "    print(\" Task 1: Sentiment Labeling\")\n",
    "    print(\" Task 2: Exploratory Data Analysis\")\n",
    "    print(\" Task 3: Employee Score Calculation\")\n",
    "    print(\" Task 4: Employee Ranking\")\n",
    "    print(\" Task 5: Flight Risk Identification (CORRECTED)\")\n",
    "    print(\" Task 6: Predictive Modeling\")\n",
    "    print(\" Documentation and Visualizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
